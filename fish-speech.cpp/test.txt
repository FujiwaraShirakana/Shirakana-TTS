{
  embeddings Embedding(num_embeddings[32000], embedding_dim[4096]) = {
    embeddings.weight (Size[32000, 4096])
  }
  layers = {
    layers.0 TransformerBlock() = {
      layers.0.attention AttentionLayer() = {
        layers.0.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.0.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.0.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.0.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.0.feed_forward FeedForward() = {
        layers.0.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.0.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.0.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.0.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.0.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.0.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.0.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.0.ffn_norm.weight (Size[4096])
      }
      layers.0.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.0.attention_norm.weight (Size[4096])
      }
    }
    layers.1 TransformerBlock() = {
      layers.1.attention AttentionLayer() = {
        layers.1.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.1.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.1.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.1.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.1.feed_forward FeedForward() = {
        layers.1.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.1.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.1.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.1.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.1.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.1.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.1.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.1.ffn_norm.weight (Size[4096])
      }
      layers.1.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.1.attention_norm.weight (Size[4096])
      }
    }
    layers.28 TransformerBlock() = {
      layers.28.attention AttentionLayer() = {
        layers.28.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.28.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.28.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.28.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.28.feed_forward FeedForward() = {
        layers.28.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.28.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.28.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.28.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.28.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.28.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.28.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.28.ffn_norm.weight (Size[4096])
      }
      layers.28.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.28.attention_norm.weight (Size[4096])
      }
    }
    layers.2 TransformerBlock() = {
      layers.2.attention AttentionLayer() = {
        layers.2.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.2.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.2.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.2.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.2.feed_forward FeedForward() = {
        layers.2.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.2.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.2.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.2.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.2.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.2.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.2.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.2.ffn_norm.weight (Size[4096])
      }
      layers.2.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.2.attention_norm.weight (Size[4096])
      }
    }
    layers.29 TransformerBlock() = {
      layers.29.attention AttentionLayer() = {
        layers.29.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.29.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.29.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.29.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.29.feed_forward FeedForward() = {
        layers.29.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.29.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.29.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.29.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.29.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.29.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.29.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.29.ffn_norm.weight (Size[4096])
      }
      layers.29.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.29.attention_norm.weight (Size[4096])
      }
    }
    layers.3 TransformerBlock() = {
      layers.3.attention AttentionLayer() = {
        layers.3.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.3.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.3.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.3.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.3.feed_forward FeedForward() = {
        layers.3.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.3.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.3.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.3.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.3.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.3.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.3.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.3.ffn_norm.weight (Size[4096])
      }
      layers.3.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.3.attention_norm.weight (Size[4096])
      }
    }
    layers.4 TransformerBlock() = {
      layers.4.attention AttentionLayer() = {
        layers.4.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.4.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.4.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.4.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.4.feed_forward FeedForward() = {
        layers.4.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.4.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.4.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.4.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.4.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.4.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.4.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.4.ffn_norm.weight (Size[4096])
      }
      layers.4.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.4.attention_norm.weight (Size[4096])
      }
    }
    layers.5 TransformerBlock() = {
      layers.5.attention AttentionLayer() = {
        layers.5.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.5.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.5.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.5.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.5.feed_forward FeedForward() = {
        layers.5.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.5.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.5.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.5.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.5.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.5.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.5.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.5.ffn_norm.weight (Size[4096])
      }
      layers.5.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.5.attention_norm.weight (Size[4096])
      }
    }
    layers.6 TransformerBlock() = {
      layers.6.attention AttentionLayer() = {
        layers.6.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.6.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.6.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.6.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.6.feed_forward FeedForward() = {
        layers.6.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.6.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.6.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.6.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.6.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.6.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.6.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.6.ffn_norm.weight (Size[4096])
      }
      layers.6.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.6.attention_norm.weight (Size[4096])
      }
    }
    layers.7 TransformerBlock() = {
      layers.7.attention AttentionLayer() = {
        layers.7.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.7.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.7.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.7.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.7.feed_forward FeedForward() = {
        layers.7.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.7.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.7.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.7.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.7.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.7.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.7.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.7.ffn_norm.weight (Size[4096])
      }
      layers.7.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.7.attention_norm.weight (Size[4096])
      }
    }
    layers.22 TransformerBlock() = {
      layers.22.attention AttentionLayer() = {
        layers.22.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.22.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.22.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.22.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.22.feed_forward FeedForward() = {
        layers.22.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.22.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.22.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.22.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.22.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.22.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.22.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.22.ffn_norm.weight (Size[4096])
      }
      layers.22.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.22.attention_norm.weight (Size[4096])
      }
    }
    layers.8 TransformerBlock() = {
      layers.8.attention AttentionLayer() = {
        layers.8.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.8.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.8.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.8.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.8.feed_forward FeedForward() = {
        layers.8.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.8.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.8.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.8.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.8.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.8.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.8.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.8.ffn_norm.weight (Size[4096])
      }
      layers.8.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.8.attention_norm.weight (Size[4096])
      }
    }
    layers.23 TransformerBlock() = {
      layers.23.attention AttentionLayer() = {
        layers.23.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.23.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.23.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.23.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.23.feed_forward FeedForward() = {
        layers.23.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.23.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.23.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.23.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.23.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.23.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.23.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.23.ffn_norm.weight (Size[4096])
      }
      layers.23.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.23.attention_norm.weight (Size[4096])
      }
    }
    layers.9 TransformerBlock() = {
      layers.9.attention AttentionLayer() = {
        layers.9.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.9.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.9.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.9.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.9.feed_forward FeedForward() = {
        layers.9.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.9.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.9.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.9.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.9.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.9.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.9.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.9.ffn_norm.weight (Size[4096])
      }
      layers.9.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.9.attention_norm.weight (Size[4096])
      }
    }
    layers.10 TransformerBlock() = {
      layers.10.attention AttentionLayer() = {
        layers.10.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.10.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.10.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.10.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.10.feed_forward FeedForward() = {
        layers.10.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.10.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.10.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.10.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.10.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.10.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.10.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.10.ffn_norm.weight (Size[4096])
      }
      layers.10.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.10.attention_norm.weight (Size[4096])
      }
    }
    layers.11 TransformerBlock() = {
      layers.11.attention AttentionLayer() = {
        layers.11.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.11.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.11.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.11.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.11.feed_forward FeedForward() = {
        layers.11.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.11.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.11.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.11.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.11.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.11.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.11.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.11.ffn_norm.weight (Size[4096])
      }
      layers.11.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.11.attention_norm.weight (Size[4096])
      }
    }
    layers.12 TransformerBlock() = {
      layers.12.attention AttentionLayer() = {
        layers.12.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.12.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.12.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.12.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.12.feed_forward FeedForward() = {
        layers.12.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.12.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.12.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.12.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.12.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.12.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.12.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.12.ffn_norm.weight (Size[4096])
      }
      layers.12.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.12.attention_norm.weight (Size[4096])
      }
    }
    layers.13 TransformerBlock() = {
      layers.13.attention AttentionLayer() = {
        layers.13.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.13.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.13.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.13.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.13.feed_forward FeedForward() = {
        layers.13.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.13.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.13.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.13.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.13.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.13.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.13.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.13.ffn_norm.weight (Size[4096])
      }
      layers.13.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.13.attention_norm.weight (Size[4096])
      }
    }
    layers.14 TransformerBlock() = {
      layers.14.attention AttentionLayer() = {
        layers.14.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.14.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.14.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.14.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.14.feed_forward FeedForward() = {
        layers.14.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.14.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.14.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.14.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.14.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.14.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.14.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.14.ffn_norm.weight (Size[4096])
      }
      layers.14.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.14.attention_norm.weight (Size[4096])
      }
    }
    layers.15 TransformerBlock() = {
      layers.15.attention AttentionLayer() = {
        layers.15.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.15.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.15.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.15.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.15.feed_forward FeedForward() = {
        layers.15.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.15.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.15.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.15.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.15.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.15.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.15.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.15.ffn_norm.weight (Size[4096])
      }
      layers.15.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.15.attention_norm.weight (Size[4096])
      }
    }
    layers.16 TransformerBlock() = {
      layers.16.attention AttentionLayer() = {
        layers.16.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.16.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.16.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.16.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.16.feed_forward FeedForward() = {
        layers.16.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.16.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.16.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.16.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.16.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.16.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.16.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.16.ffn_norm.weight (Size[4096])
      }
      layers.16.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.16.attention_norm.weight (Size[4096])
      }
    }
    layers.17 TransformerBlock() = {
      layers.17.attention AttentionLayer() = {
        layers.17.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.17.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.17.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.17.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.17.feed_forward FeedForward() = {
        layers.17.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.17.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.17.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.17.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.17.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.17.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.17.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.17.ffn_norm.weight (Size[4096])
      }
      layers.17.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.17.attention_norm.weight (Size[4096])
      }
    }
    layers.18 TransformerBlock() = {
      layers.18.attention AttentionLayer() = {
        layers.18.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.18.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.18.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.18.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.18.feed_forward FeedForward() = {
        layers.18.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.18.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.18.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.18.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.18.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.18.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.18.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.18.ffn_norm.weight (Size[4096])
      }
      layers.18.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.18.attention_norm.weight (Size[4096])
      }
    }
    layers.19 TransformerBlock() = {
      layers.19.attention AttentionLayer() = {
        layers.19.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.19.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.19.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.19.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.19.feed_forward FeedForward() = {
        layers.19.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.19.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.19.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.19.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.19.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.19.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.19.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.19.ffn_norm.weight (Size[4096])
      }
      layers.19.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.19.attention_norm.weight (Size[4096])
      }
    }
    layers.20 TransformerBlock() = {
      layers.20.attention AttentionLayer() = {
        layers.20.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.20.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.20.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.20.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.20.feed_forward FeedForward() = {
        layers.20.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.20.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.20.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.20.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.20.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.20.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.20.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.20.ffn_norm.weight (Size[4096])
      }
      layers.20.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.20.attention_norm.weight (Size[4096])
      }
    }
    layers.21 TransformerBlock() = {
      layers.21.attention AttentionLayer() = {
        layers.21.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.21.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.21.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.21.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.21.feed_forward FeedForward() = {
        layers.21.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.21.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.21.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.21.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.21.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.21.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.21.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.21.ffn_norm.weight (Size[4096])
      }
      layers.21.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.21.attention_norm.weight (Size[4096])
      }
    }
    layers.24 TransformerBlock() = {
      layers.24.attention AttentionLayer() = {
        layers.24.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.24.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.24.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.24.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.24.feed_forward FeedForward() = {
        layers.24.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.24.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.24.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.24.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.24.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.24.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.24.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.24.ffn_norm.weight (Size[4096])
      }
      layers.24.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.24.attention_norm.weight (Size[4096])
      }
    }
    layers.25 TransformerBlock() = {
      layers.25.attention AttentionLayer() = {
        layers.25.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.25.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.25.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.25.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.25.feed_forward FeedForward() = {
        layers.25.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.25.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.25.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.25.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.25.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.25.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.25.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.25.ffn_norm.weight (Size[4096])
      }
      layers.25.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.25.attention_norm.weight (Size[4096])
      }
    }
    layers.26 TransformerBlock() = {
      layers.26.attention AttentionLayer() = {
        layers.26.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.26.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.26.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.26.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.26.feed_forward FeedForward() = {
        layers.26.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.26.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.26.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.26.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.26.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.26.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.26.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.26.ffn_norm.weight (Size[4096])
      }
      layers.26.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.26.attention_norm.weight (Size[4096])
      }
    }
    layers.27 TransformerBlock() = {
      layers.27.attention AttentionLayer() = {
        layers.27.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.27.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.27.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.27.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.27.feed_forward FeedForward() = {
        layers.27.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.27.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.27.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.27.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.27.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.27.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.27.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.27.ffn_norm.weight (Size[4096])
      }
      layers.27.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.27.attention_norm.weight (Size[4096])
      }
    }
    layers.30 TransformerBlock() = {
      layers.30.attention AttentionLayer() = {
        layers.30.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.30.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.30.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.30.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.30.feed_forward FeedForward() = {
        layers.30.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.30.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.30.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.30.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.30.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.30.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.30.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.30.ffn_norm.weight (Size[4096])
      }
      layers.30.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.30.attention_norm.weight (Size[4096])
      }
    }
    layers.31 TransformerBlock() = {
      layers.31.attention AttentionLayer() = {
        layers.31.attention.wqkv Linear(in_feature[4096], out_feature[1920]) = {
          layers.31.attention.wqkv.weight (Size[1920, 4096])
        }
        layers.31.attention.wo Linear(in_feature[4096], out_feature[4096]) = {
          layers.31.attention.wo.weight (Size[4096, 4096])
        }
      }
      layers.31.feed_forward FeedForward() = {
        layers.31.feed_forward.w1 Linear(in_feature[4096], out_feature[1]) = {
          layers.31.feed_forward.w1.weight (Size[1, 4096])
        }
        layers.31.feed_forward.w3 Linear(in_feature[4096], out_feature[1]) = {
          layers.31.feed_forward.w3.weight (Size[1, 4096])
        }
        layers.31.feed_forward.w2 Linear(in_feature[1], out_feature[4096]) = {
          layers.31.feed_forward.w2.weight (Size[4096, 1])
        }
      }
      layers.31.ffn_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.31.ffn_norm.weight (Size[4096])
      }
      layers.31.attention_norm RMSNorm(dim[4096], eps[1e-05]) = {
        layers.31.attention_norm.weight (Size[4096])
      }
    }
  }
  norm RMSNorm(dim[4096], eps[1e-05]) = {
    norm.weight (Size[4096])
  }
  output Linear(in_feature[4096], out_feature[32000]) = {
    output.weight (Size[32000, 4096])
  }
  freqs_cis (Size[2048, 64])
  causal_mask (Size[2048, 2048])
}